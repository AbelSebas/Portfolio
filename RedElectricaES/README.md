üìä Sistema Automatizado de Extracci√≥n y Carga de Datos de Demanda REE

Este proyecto implementa una arquitectura automatizada y escalable para la extracci√≥n, procesamiento y almacenamiento de datos de demanda el√©ctrica provenientes de la Red El√©ctrica de Espa√±a (REE).

El sistema permite obtener datos hist√≥ricos y actualizados de demanda, almacenarlos en AWS S3 y cargarlos de forma estructurada en una base de datos PostgreSQL para su posterior an√°lisis.

üöÄ Objetivo

Construir una pipeline de datos automatizada que extraiga informaci√≥n de la API de la REE, la almacene en un entorno seguro en la nube y la procese para su an√°lisis eficiente en una base de datos relacional.

‚öôÔ∏è Arquitectura General

Extracci√≥n de Datos (Data Engineering & Architecture):

Obtenci√≥n de la API Key de REE.

Desarrollo de una pipeline de extracci√≥n masiva para recopilar datos hist√≥ricos de demanda y almacenarlos en un bucket S3.

Configuraci√≥n de una AWS Lambda programada mediante EventBridge para ejecutar la extracci√≥n diaria de las √∫ltimas 24 horas y guardar las observaciones en el bucket.

Procesamiento y Carga (ETL - Data Engineering):

Implementaci√≥n de una segunda AWS Lambda con trigger de S3, que se activa al detectar nuevos ficheros JSON.

Esta funci√≥n procesa y limpia los datos antes de cargarlos en una base de datos PostgreSQL, asegurando integridad y consistencia en el almacenamiento.

üß© Tecnolog√≠as Utilizadas

AWS Lambda

Amazon S3

Amazon EventBridge

PostgreSQL

Python (boto3, psycopg2, pandas)

API REE (Red El√©ctrica de Espa√±a)

üìà Beneficios

Pipeline completamente automatizada y sin servidor (serverless).

Almacenamiento de datos hist√≥rico y actualizado.

Estructura escalable y mantenible.

Datos limpios y listos para an√°lisis o visualizaci√≥n.

FICHEROS CON LAS DIFERENTES PARTES DEL PROYECTO
## 00.Ree
En este archivo se hace un request de todos los datos disponibles a la API de REE - 31 diciembre 2013-. Pasos:
- Petici√≥n de Token a la API de REE
- Con este token sacamos la demanda de la Pen√≠nsula por d√≠a, base url + id + params

## 01. Ree_indicators
Para saber que indicador ten√≠amos que usar en el archivo anterior.

## 02. Extracion_load
Este archivo saca los datos y los lleva al bucket AWS S3 donde se guardan por a√±o/mes/nombre_del_archivo_fecha.json

## 03. Merged_files
Este script busca todos los archivos en las carpetas donde se guardan los datos de demanda por d√≠a y los une en un √∫nico json para su carga en RDS.

## 04. BBDD_creation
Creaci√≥n de base de datos donde se importa el archivo generado en el notebook anterior para guardarlo en la BBDD.

## 05. Model_training_funciones
Version de testeo para la FastAPI. En este notebook entrenamos los modelos XGBoost (forescasting) que predicir√°n t+1, t+2, t+3. 
Se ajustan features y se prueban para convertirlo en la funci√≥n externa .py y generar los modelos plks.

## Text_to_SQL_hf
Versi√≥n de testeo para la FastAPI. En este notebook realizamos una traducci√≥n de frases coloquiales a consultas de SQL a trav√©s de un modelo HuggingFace "defog/llama-3-sqlcoder-8b":
- Introducci√≥n frase en espa√±ol
- Se traduce mediante Google Translator
- Devuelve una consulta SQL mediante un prompt definido con instrucciones (limitaciones).


## S3-Trigger-RDS-versionfinal
Lambda que recoge nuevos datos de la API.
Lambda que identifica un evento en S3 de subida de un nuevo archivo ‚Äìdemanda del d√≠a anterior‚Äì y lo pasa a la base de datos en RDS.

## FastAPI-Energy
Estructura de FastAPI que est√° levantada en EC2. Archivos definitivos para hacer endpoint de /forecasting y /ask con los modelos anteriormente citados y el conversor de texto a SQL.
